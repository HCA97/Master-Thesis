{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade8f4be",
   "metadata": {},
   "source": [
    "## ARTIFICIAL CARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import utils\n",
    "import torch as th\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts import *\n",
    "\n",
    "potsdam_dir = \"/home/hca/Documents/Master/Thesis/potsdam_data/cem-v0/v2/training_tightcanvas_graybackground\"\n",
    "\n",
    "transform = transforms.Compose([#transforms.ColorJitter(hue=[-0.1, 0.1]),\n",
    "                                DynamicPad(min_img_dim=(120, 60),\n",
    "                                           padding_mode=\"constant\", padding_value=125),\n",
    "                                transforms.Resize((60, 120)),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomVerticalFlip(p=0.5),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "datasetmodule = PostdamCarsDataModule(potsdam_dir, batch_size=64, transform=transform)\n",
    "datasetmodule.setup()\n",
    "dataloader = datasetmodule.train_dataloader()\n",
    "\n",
    "real, _ = next(iter(dataloader))\n",
    "grid = utils.make_grid(real, normalize=True, normalize_range=(-1, 1), pad_value=1)\n",
    "utils.save_image(grid, \"figs/artificial_cars_munit_gray.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb3b2c1",
   "metadata": {},
   "source": [
    "## POTSDAM VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b34fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "training_img  = \"/home/hca/Documents/Master/Thesis/potsdam_data/training/top_potsdam_7_7_RGB.tif\"\n",
    "training_mask =  \"/home/hca/Documents/Master/Thesis/potsdam_data/training/top_potsdam_7_7_label.tif\"\n",
    "\n",
    "test_img  = \"/home/hca/Documents/Master/Thesis/potsdam_data/test/top_potsdam_4_13_RGB.tif\"\n",
    "test_mask = \"/home/hca/Documents/Master/Thesis/potsdam_data/test/top_potsdam_4_13_label.tif\"\n",
    "\n",
    "\n",
    "train = cv2.imread(training_img)\n",
    "mask = cv2.imread(training_mask)\n",
    "\n",
    "w, h = 3000, 3000\n",
    "w1, h1 = 100, 200\n",
    "\n",
    "small_train = train[w:w+2000, h:h+2000]\n",
    "small_mask = mask[w:w+2000, h:h+2000]\n",
    "small_train = cv2.cvtColor(small_train, cv2.COLOR_BGR2RGB)\n",
    "small_mask = cv2.cvtColor(small_mask, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(small_train)\n",
    "plt.gca().add_patch(Rectangle((h1,w1),400,400,linewidth=5,edgecolor=\"k\",facecolor='none'))\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(small_mask)\n",
    "plt.gca().add_patch(Rectangle((h1,w1),400,400,linewidth=5,edgecolor='k',facecolor='none'))\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/small_train.png\", dpi=300)\n",
    "\n",
    "small_train_ = small_train[w1:w1+400, h1:h1+400]\n",
    "small_mask_ = small_mask[w1:w1+400, h1:h1+400]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(small_train_)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(small_mask_)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/small_train_zoom.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316957c",
   "metadata": {},
   "source": [
    "## BORDER ARTIFACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf68848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch as th\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from scripts.callbacks import *\n",
    "from scripts.models import *\n",
    "from scripts.dataloader import *\n",
    "\n",
    "\n",
    "@ th.no_grad()\n",
    "def plot_generator_steps(z, pl_module):\n",
    "    pl_module.eval()\n",
    "\n",
    "    # Generator Inner Layers\n",
    "    init_channels = pl_module.generator.init_channels\n",
    "    init_height = pl_module.generator.init_height\n",
    "    init_width = pl_module.generator.init_width\n",
    "    x = pl_module.generator.l1(z).reshape(\n",
    "        1, init_channels, init_height, init_width).detach()\n",
    "    gen_inter_layers = []\n",
    "    layer_i = 0\n",
    "    for layer in pl_module.generator.conv_blocks[:-1]:\n",
    "        x = layer(x)\n",
    "        if layer.__class__.__name__ in [\"ConvBlock\", \"BatchNorm2d\", \"ResBlock\"]:\n",
    "            gen_inter_layers.append(x)\n",
    "    x = pl_module.generator.conv_blocks[-1](x)\n",
    "    return x, gen_inter_layers[-1]\n",
    "\n",
    "\n",
    "checkpoint_path = \"experiments/dcgan_experiments/lightning_logs/version_0/checkpoints/epoch=449.ckpt\"\n",
    "\n",
    "# load model\n",
    "model = GAN.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "with th.no_grad():\n",
    "    z = th.normal(0, 1, (1, model.generator.latent_dim),\n",
    "                  device=model.device)\n",
    "    fake_image, last_activation = plot_generator_steps(z, model)\n",
    "\n",
    "    x = fake_image.detach().clone()\n",
    "    for i, layer in enumerate(model.discriminator.conv_blocks):\n",
    "        x = layer(x)\n",
    "        if layer.__class__.__name__ in [\"ConvBlock\", \"AvgPool2d\"]:\n",
    "            break\n",
    "    first_activation = x.detach().clone()\n",
    "    \n",
    "first_activation_grid =  torchvision.utils.make_grid(first_activation.permute(1, 0, 2, 3), \n",
    "                                                     nrow=8, padding=1, \n",
    "                                                     normalize=True, pad_value=1).numpy().transpose((1, 2, 0))   \n",
    "# make numpy\n",
    "fake_image = (th.squeeze(fake_image.detach().clone().cpu().permute(0, 2, 3, 1)).numpy() + 1) / 2\n",
    "last_activation = last_activation.detach().clone().cpu().permute(1, 2, 3, 0).numpy() \n",
    "last_activation /= np.max(last_activation, axis=(1,2,3)).reshape(len(last_activation), 1, 1, 1)\n",
    "first_activation = first_activation.detach().clone().cpu().permute(1, 2, 3, 0).numpy()\n",
    "first_activation /= np.max(first_activation, axis=(1,2,3)).reshape(len(first_activation), 1, 1, 1)\n",
    "\n",
    "plt.figure(figsize=(18, 9))\n",
    "rec1 = patches.Rectangle((6*32 + 5*1 + 1, 0), 32.5, 16.5,linewidth=3,edgecolor='r',facecolor='none', linestyle=\"-\",)\n",
    "rec2 = patches.Rectangle((0, 2*16 + 2), 32.5, 16.5,linewidth=3,edgecolor='b',facecolor='none', linestyle=\"-\",)\n",
    "plt.gca().add_patch(rec1)\n",
    "plt.gca().add_patch(rec2)\n",
    "plt.imshow(first_activation_grid)\n",
    "plt.axis(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/first_activation.png\", dpi=300)\n",
    "\n",
    "plt.figure(figsize=(18, 9))\n",
    "plt.imshow(fake_image)\n",
    "plt.axis(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/fake_image.png\", dpi=300)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "rec1 = patches.Rectangle((-0.5,-0.5), 32, 16,linewidth=10,edgecolor='r',facecolor='none', linestyle=\"-\",)\n",
    "plt.gca().add_patch(rec1)\n",
    "plt.imshow(first_activation[6], cmap=\"gray\")\n",
    "plt.axis(False)\n",
    "plt.subplot(1, 2, 2)\n",
    "rec1 = patches.Rectangle((-0.5,-0.5), 32, 16,linewidth=10,edgecolor='b',facecolor='none', linestyle=\"-\",)\n",
    "plt.gca().add_patch(rec1)\n",
    "plt.imshow(first_activation[16], cmap=\"gray\")\n",
    "plt.axis(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/zoomed_activations.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5a8c9",
   "metadata": {},
   "source": [
    "## FID CURVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# save plots for latex\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "fid_0 = \"experiments/dcgan_experiments/version_0/results/fid_scores.npy\"\n",
    "fid_1 = \"experiments/dcgan_experiments/version_1/results/fid_scores.npy\"\n",
    "fid_2 = \"experiments/dcgan_experiments/version_2/results/fid_scores.npy\"\n",
    "\n",
    "# load the data\n",
    "fid_0_ = np.load(fid_0, allow_pickle=True).item()[\"fid_train\"]\n",
    "fid_1_ = np.load(fid_1, allow_pickle=True).item()[\"fid_train\"]\n",
    "fid_2_ = np.load(fid_2, allow_pickle=True).item()[\"fid_train\"]\n",
    "epochs = [i-1 for i in range(25, 1000, 25)] + [1000-1]\n",
    "plot_idx = [i for i in range(0, len(fid_2_), 5)] + [len(fid_2_)-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xticks(plot_idx, [epochs[i] for i in plot_idx], fontsize=20)\n",
    "plt.yticks([i for i in range(0, 10)], [i for i in range(0, 10)], fontsize=20)\n",
    "plt.ylim([0, 10])\n",
    "plt.xlim([-0.5, 40.5])\n",
    "plt.ylabel(\"FID Score\", fontsize=22)\n",
    "plt.xlabel(\"Epochs\", fontsize=22)\n",
    "plt.plot(fid_0_, \"--\", marker=\"^\", linewidth=3, label=\"A Baseline\")\n",
    "plt.plot(fid_1_, \"--\", marker=\"*\", linewidth=3, label=\"B + Padding Reflect\")\n",
    "plt.plot(fid_2_, \"--\", marker=\"o\", linewidth=3, label=\"C + LR Scheduler\")\n",
    "plt.plot( [-10, len(fid_2_)+10], [1.33, 1.33], \"--k\", linewidth=3, label=\"Training Data\")\n",
    "plt.legend(prop={'size': 16})\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/fid_curves.png\", dpi=300)\n",
    "plt.savefig(\"figs/fid_curves.pgf\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf7ccf",
   "metadata": {},
   "source": [
    "## VISUAL COMPARISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch as th\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from scripts.callbacks import *\n",
    "from scripts.models import *\n",
    "from scripts.dataloader import *\n",
    "\n",
    "model1 = GAN.load_from_checkpoint(\"experiments/dcgan_experiments/lightning_logs/version_0/checkpoints/epoch=449.ckpt\")\n",
    "model1.eval()\n",
    "model2 = GAN.load_from_checkpoint(\"experiments/dcgan_experiments/lightning_logs/version_1/checkpoints/epoch=524.ckpt\")\n",
    "model2.eval()\n",
    "model3 = GAN.load_from_checkpoint(\"experiments/dcgan_experiments/lightning_logs/version_2/checkpoints/epoch=974.ckpt\")\n",
    "model3.eval()\n",
    "model4 = GAN.load_from_checkpoint(\"experiments/dcgan_experiments_500/lightning_logs/version_0/checkpoints/epoch=1294.ckpt\")\n",
    "model4.eval()\n",
    "\n",
    "\n",
    "with th.no_grad():\n",
    "    gen_in = th.normal(0, 1, size=(30, 100))\n",
    "    fake = model1(gen_in)\n",
    "    grid =  torchvision.utils.make_grid(fake, nrow=10, padding=1, normalize=True, pad_value=1)\n",
    "    torchvision.utils.save_image(grid, \"figs/modelA_images.png\")\n",
    "    \n",
    "    gen_in = th.normal(0, 1, size=(30, 100))\n",
    "    fake = model2(gen_in)\n",
    "    grid =  torchvision.utils.make_grid(fake, nrow=10, padding=1, normalize=True, pad_value=1)\n",
    "    torchvision.utils.save_image(grid, \"figs/modelB_images.png\")\n",
    "    \n",
    "    gen_in = th.normal(0, 1, size=(30, 100))\n",
    "    fake = model3(gen_in)\n",
    "    grid =  torchvision.utils.make_grid(fake, nrow=10, padding=1, normalize=True, pad_value=1)\n",
    "    torchvision.utils.save_image(grid, \"figs/modelC_images.png\")\n",
    "    \n",
    "    gen_in = th.normal(0, 1, size=(30, 100))\n",
    "    fake = model4(gen_in)\n",
    "    grid =  torchvision.utils.make_grid(fake, nrow=10, padding=1, normalize=True, pad_value=1)\n",
    "    torchvision.utils.save_image(grid, \"figs/modelD_images.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69ee27",
   "metadata": {},
   "source": [
    "## CLOSTEST MATCH ON REAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch as th\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from scripts.callbacks import *\n",
    "from scripts.models import *\n",
    "from scripts.dataloader import *\n",
    "\n",
    "\n",
    "with th.no_grad():\n",
    "    model = GAN.load_from_checkpoint(\"experiments/dcgan_experiments/lightning_logs/version_2/checkpoints/epoch=974.ckpt\")\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    gen_in = th.normal(0, 1, size=(20, 100))\n",
    "    fake = model(gen_in.cuda()).detach().cpu().clone()\n",
    "    act2 = np.squeeze(vgg16_get_activation_maps(\n",
    "        fake, layer_idx=33, device=\"cuda:0\", normalize_range=(-1, 1)).numpy())\n",
    "\n",
    "\n",
    "    potsdam_dir = \"../potsdam_data/potsdam_cars_corrected\"\n",
    "    dataset = PostdamCarsDataModule(potsdam_dir, batch_size=4300)\n",
    "    dataset.setup()\n",
    "    potsdam_cars_dataloader = dataset.train_dataloader()\n",
    "\n",
    "    real,_ = next(iter(potsdam_cars_dataloader))\n",
    "    act1 = np.squeeze(vgg16_get_activation_maps(\n",
    "        real, layer_idx=33, device=\"cuda:0\", normalize_range=(-1, 1)).cpu().numpy())\n",
    "\n",
    "\n",
    "    act1_norm = np.expand_dims(np.linalg.norm(act1, axis=1), 1)\n",
    "    act2_norm = np.expand_dims(np.linalg.norm(act2, axis=1), 1)\n",
    "    norms_inverse = 1 / (act2_norm.dot(act1_norm.T))\n",
    "\n",
    "    norms_inverse[np.isinf(norms_inverse)] = 0\n",
    "\n",
    "    # cos distance between two activations\n",
    "    cos_distances = (1 - norms_inverse*act2.dot(act1.T))\n",
    "    cos_distance = cos_distances.min(axis=1)  # min for each row\n",
    "\n",
    "    jj = np.argsort(cos_distance) # best\n",
    "    n = 10\n",
    "    imgs = []\n",
    "    for j in jj:\n",
    "        idx = np.argsort(cos_distances[j])\n",
    "\n",
    "        imgs.append(fake[j])\n",
    "        for idx_ in idx[:n]:\n",
    "            imgs.append(real[idx_])\n",
    "            \n",
    "    grid = torchvision.utils.make_grid(imgs, nrow=n+1, padding=1, normalize=True, pad_value=1)\n",
    "    torchvision.utils.save_image(grid, \"figs/modelC_best.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecdd0da",
   "metadata": {},
   "source": [
    "## PCA DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9010bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch as th \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision;\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from scripts import *\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# save plots for latex\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "save_path = \"experiments/ganspace/dcgan_experiments_version_2_epoch=974\"\n",
    "layer=\"linear\"\n",
    "n_time=5\n",
    "pca_path = os.path.join(save_path, layer, \"pca.pkl\")\n",
    "\n",
    "f = open(pca_path, 'rb')\n",
    "pca = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# load dataset\n",
    "X = np.concatenate([np.load(os.path.join(save_path, layer, f\"X{i+1}.npy\")) for i in range(n_time)], axis=0)\n",
    "y = np.concatenate([np.load(os.path.join(save_path, layer, f\"y{i+1}.npy\")) for i in range(n_time)], axis=0)\n",
    "\n",
    "# information gain\n",
    "information_gain = np.cumsum(pca.explained_variance_ratio_)\n",
    "idx = np.where(information_gain >= 0.9)[0][0]\n",
    "idx = 12\n",
    "\n",
    "# \n",
    "comp = pca.components_[:idx]\n",
    "mean = pca.mean_\n",
    "\n",
    "# reconstruction\n",
    "X_pca = (X - mean).dot(comp.T) \n",
    "\n",
    "# resonctruction error\n",
    "X_rec = X_pca.dot(comp) + mean\n",
    "np.linalg.norm(X_rec - X) / len(X)\n",
    "\n",
    "columns = 4\n",
    "rows = 1\n",
    "fig=plt.figure(figsize=(columns*5, rows*5))\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "for i in range(1, columns*rows +1):\n",
    "    dim = X_pca[:, i-1]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    \n",
    "    # data dist\n",
    "    plt.hist(dim, bins=50, density=True, alpha=0.6)\n",
    "    \n",
    "    # fit gaus\n",
    "    mu, std = dim.mean(), dim.std()\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    plt.plot(x, p, 'k--', linewidth=2)\n",
    "    \n",
    "    plt.title(\"Principle Dimension %d\" % i, fontsize=30)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/eigen_dim_distributions.png\", dpi=300)\n",
    "plt.savefig(\"figs/eigen_dim_distributions.pgf\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667eff00",
   "metadata": {},
   "source": [
    "## PCA VARIANCE PERCENTAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch as th \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision;\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from scripts import *\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# save plots for latex\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "MAX = 50\n",
    "\n",
    "# Experiement C\n",
    "save_path = \"experiments/ganspace/dcgan_experiments_version_2_epoch=974\"\n",
    "layer=\"linear\"\n",
    "n_time=5\n",
    "pca_path = os.path.join(save_path, layer, \"pca.pkl\")\n",
    "\n",
    "f = open(pca_path, 'rb')\n",
    "pca = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "idx=30\n",
    "MAX=idx\n",
    "color=[\"blue\" if i < idx else \"red\" for i in range(MAX)]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(np.arange(2, 2*MAX+2, 2), \n",
    "        100*pca.explained_variance_ratio_[:MAX], \n",
    "        width=1.4, color=color, alpha=0.75)\n",
    "#plt.xticks(np.arange(2, 2*MAX+2, 2), 1 + np.arange(MAX), fontsize=16, rotation=90)\n",
    "plt.xticks([2, 2*idx, 2*MAX], [1, idx, MAX], fontsize=16, rotation=90)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim([0, 18])\n",
    "plt.ylabel(\"Variance Percentage (%)\", fontsize=18)\n",
    "plt.xlabel(\"Principal Components\", fontsize=18)\n",
    "plt.grid(axis='y')\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/pca_variance_percentage_C.png\", dpi=300)\n",
    "plt.savefig(\"figs/pca_variance_percentage_C.pgf\", dpi=300)\n",
    "plt.clf()\n",
    "plt.close()\n",
    "\n",
    "# Experiment D\n",
    "save_path = \"experiments/ganspace/dcgan_experiments_500_version_0_epoch=1294\"\n",
    "layer=\"linear\"\n",
    "n_time=5\n",
    "pca_path = os.path.join(save_path, layer, \"pca.pkl\")\n",
    "\n",
    "f = open(pca_path, 'rb')\n",
    "pca = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "idx=12\n",
    "MAX=idx\n",
    "color=[\"blue\" if i < idx else \"red\" for i in range(MAX)]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(np.arange(2, 2*MAX+2, 2), \n",
    "        100*pca.explained_variance_ratio_[:MAX], \n",
    "        width=1.4, color=color, alpha=0.75)\n",
    "#plt.xticks(np.arange(2, 2*MAX+2, 2), 1 + np.arange(MAX), fontsize=16, rotation=90)\n",
    "plt.xticks([2, 2*idx, 2*MAX], [1, idx, MAX], fontsize=16, rotation=90)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylabel(\"Variance Percentage (%)\", fontsize=18)\n",
    "plt.xlabel(\"Principal Components\", fontsize=18)\n",
    "plt.grid(axis='y')\n",
    "plt.ylim([0, 18])\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/pca_variance_percentage_D.png\", dpi=300)\n",
    "plt.savefig(\"figs/pca_variance_percentage_D.pgf\", dpi=300)\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e205cf71",
   "metadata": {},
   "source": [
    "## DATA SIZE DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47b77941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import norm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# save plots for latex\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "potsdam_dataset = \"../potsdam_data/potsdam_cars_corrected\"\n",
    "\n",
    "munit_cropped_images = \"experiments/munit_version_2_larger_image_corrected/version_0/10000_cars_cropped\"\n",
    "\n",
    "def get_w_h(folder):\n",
    "    w, h = [], []\n",
    "    for f in os.listdir(folder):\n",
    "        f = os.path.join(folder, f)\n",
    "        img = cv2.imread(f)\n",
    "        if img is not None:\n",
    "            ww, hh, _ = img.shape\n",
    "            w.append(ww)\n",
    "            h.append(hh)\n",
    "    return w, h\n",
    "\n",
    "\n",
    "p_w, p_h = get_w_h(potsdam_dataset)\n",
    "m_w, m_h = get_w_h(munit_cropped_images)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([32], [1], width=2, color=\"green\", alpha=0.75)\n",
    "plt.hist(m_w, bins=20, density=True, alpha=0.75)\n",
    "plt.hist(p_w, bins=50, density=True, alpha=0.75)\n",
    "plt.ylim([0, 0.2])\n",
    "\n",
    "plt.xlim([0, 80])\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(30, 60)\n",
    "mu, std = np.mean(p_w), np.std(p_w)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k--', linewidth=2)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=22)\n",
    "plt.xlabel(\"Car Width (in pixels)\", fontsize=24)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/width_distribuion.png\", dpi=300)\n",
    "plt.savefig(\"figs/width_distribuion.pgf\", dpi=300)\n",
    "plt.clf()\n",
    "plt.close()\n",
    "\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.bar([64], [1], width=5, color=\"green\", alpha=0.75)\n",
    "plt.hist(m_h, bins=20, density=True, alpha=0.75)\n",
    "plt.hist(p_h, bins=50, density=True, alpha=0.75)\n",
    "plt.xlim([0, 200])\n",
    "plt.ylim([0, 0.07])\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(50, 150)\n",
    "mu, std = np.mean(p_h), np.std(p_h)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k--', linewidth=2)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.yticks([])\n",
    "plt.xticks(fontsize=22)\n",
    "plt.xlabel(\"Car Length (in pixels)\", fontsize=24)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/height_distribution.png\", dpi=300)\n",
    "plt.savefig(\"figs/height_distribution.pgf\", dpi=300)\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea903f2",
   "metadata": {},
   "source": [
    "## Style Mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a4819a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import utils\n",
    "import torch as th\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts import *\n",
    "\n",
    "potsdam_dir = \"../potsdam_data/potsdam_cars_corrected\"\n",
    "artificial_dir = \"../potsdam_data/cem-v0/v2/training_tightcanvas_graybackground\"\n",
    "img_dim = (3, 40, 80)\n",
    "\n",
    "# DATA AUG FOR\n",
    "transform1 = transforms.Compose([transforms.ColorJitter(hue=[-0.1, 0.1]),\n",
    "                                 DynamicPad(min_img_dim=(110, 60),\n",
    "                                            padding_mode=\"edge\"),\n",
    "                                 transforms.RandomCrop(\n",
    "                                     (55, 105), padding_mode=\"reflect\"),\n",
    "                                 transforms.Resize(img_dim[1:]),\n",
    "                                 transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                 transforms.RandomVerticalFlip(p=0.5),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "transform2 = transforms.Compose([transforms.ColorJitter(hue=[-0.1, 0.1]),\n",
    "                                 DynamicPad(min_img_dim=(130, 70),\n",
    "                                            padding_mode=\"constant\", padding_value=125),\n",
    "                                 transforms.RandomRotation(\n",
    "                                     degrees=5, resample=PIL.Image.NEAREST, fill=125),\n",
    "                                 transforms.RandomCrop(\n",
    "                                     (60, 120), padding_mode=\"reflect\"),\n",
    "                                 transforms.Resize(img_dim[1:]),\n",
    "                                 transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                 transforms.RandomVerticalFlip(p=0.5),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize([0.5], [0.5]),\n",
    "                                 AddNoise(alpha=0.07)])\n",
    "\n",
    "\n",
    "potsdam = PostdamCarsDataModule(\n",
    "    potsdam_dir, img_size=img_dim[1:], batch_size=5,\n",
    "    data_dir2=artificial_dir, transform=transform1, transform2=transform2)\n",
    "potsdam.setup()\n",
    "dataloader = potsdam.train_dataloader()\n",
    "\n",
    "# style and content vector\n",
    "real, artificial = next(iter(dataloader))\n",
    "\n",
    "\n",
    "# munit1\n",
    "path = \"experiments/munit_version_2_larger_image_corrected/lightning_logs/version_1/checkpoints/epoch=899.ckpt\"\n",
    "model = MUNIT.load_from_checkpoint(path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "encoder1 = model.generator.encoder1\n",
    "encoder2 = model.generator.encoder2\n",
    "decoder1 = model.generator.decoder1\n",
    "decoder2 = model.generator.decoder2\n",
    "img_styles = real.to(model.device)\n",
    "img_contents = artificial.to(model.device)\n",
    "\n",
    "contents, _ = encoder1(img_contents)\n",
    "_, styles = encoder2(img_styles)\n",
    "\n",
    "# image styles will be the first row\n",
    "grid = [th.ones(real.shape[1:], device=\"cpu\")] + \\\n",
    "    [img_style.detach().cpu().clone()\n",
    "     for img_style in img_styles]\n",
    "\n",
    "for i, content in enumerate(contents):\n",
    "    # for each content create n_random_styles\n",
    "    row = th.cat(\n",
    "        [th.unsqueeze(content, 0) for _ in range(len(img_styles))], dim=0)\n",
    "    imgs = decoder2(row.to(model.device),\n",
    "                    styles.to(model.device))\n",
    "\n",
    "    # a row consists of content image and its style mixing\n",
    "    grid.append(img_contents[i].detach().cpu().clone())\n",
    "    grid += [img.detach().cpu().clone()\n",
    "             for img in imgs]\n",
    "\n",
    "# save everyting\n",
    "grid = torchvision.utils.make_grid(\n",
    "    grid, nrow=len(img_styles) + 1, pad_value=1, padding=7, normalize=True, range=(-1, 1))\n",
    "torchvision.utils.save_image(grid, \"figs/style_mixing_munit1.png\")\n",
    "\n",
    "\n",
    "\n",
    "# munit2\n",
    "path = \"experiments/munit_version_2_larger_image_corrected_500/lightning_logs/version_0/checkpoints/epoch=499.ckpt\"\n",
    "model = MUNIT.load_from_checkpoint(path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "encoder1 = model.generator.encoder1\n",
    "encoder2 = model.generator.encoder2\n",
    "decoder1 = model.generator.decoder1\n",
    "decoder2 = model.generator.decoder2\n",
    "img_styles = real.to(model.device)\n",
    "img_contents = artificial.to(model.device)\n",
    "\n",
    "contents, _ = encoder1(img_contents)\n",
    "_, styles = encoder2(img_styles)\n",
    "\n",
    "# image styles will be the first row\n",
    "grid = [th.ones(real.shape[1:], device=\"cpu\")] + \\\n",
    "    [img_style.detach().cpu().clone()\n",
    "     for img_style in img_styles]\n",
    "\n",
    "for i, content in enumerate(contents):\n",
    "    # for each content create n_random_styles\n",
    "    row = th.cat(\n",
    "        [th.unsqueeze(content, 0) for _ in range(len(img_styles))], dim=0)\n",
    "    imgs = decoder2(row.to(model.device),\n",
    "                    styles.to(model.device))\n",
    "\n",
    "    # a row consists of content image and its style mixing\n",
    "    grid.append(img_contents[i].detach().cpu().clone())\n",
    "    grid += [img.detach().cpu().clone()\n",
    "             for img in imgs]\n",
    "\n",
    "# save everyting\n",
    "grid = torchvision.utils.make_grid(\n",
    "    grid, nrow=len(img_styles) + 1, pad_value=1, padding=7, normalize=True, range=(-1, 1))\n",
    "torchvision.utils.save_image(grid, \"figs/style_mixing_munit2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33865dbd",
   "metadata": {},
   "source": [
    "## TESTING MUNIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8bb0fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import utils\n",
    "import torch as th\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts import *\n",
    "\n",
    "artificial_dir = \"../potsdam_data/cem-v0/v2/test_one_sample\"\n",
    "img_dim = (3, 40, 80)\n",
    "\n",
    "transform1 = transforms.Compose([DynamicPad(min_img_dim=(110, 60),\n",
    "                                            padding_mode=\"constant\", padding_value=125),\n",
    "                                 transforms.RandomRotation(\n",
    "                                     degrees=5, resample=PIL.Image.NEAREST, fill=125),\n",
    "                                 transforms.Resize(img_dim[1:]),\n",
    "                                 transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                 transforms.RandomVerticalFlip(p=0.5),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize([0.5], [0.5]),\n",
    "                                 AddNoise(alpha=0.07)])\n",
    "\n",
    "\n",
    "potsdam = PostdamCarsDataModule(\n",
    "    artificial_dir, img_size=img_dim[1:], batch_size=10, transform=transform1)\n",
    "potsdam.setup()\n",
    "dataloader = potsdam.train_dataloader()\n",
    "\n",
    "# style and content vector\n",
    "artificial, _ = next(iter(dataloader))\n",
    "\n",
    "random_styles = th.normal(0, 1, size=(10, 8), device=model.device)\n",
    "\n",
    "# munit1\n",
    "path = \"experiments/munit_version_2_larger_image_corrected/lightning_logs/version_1/checkpoints/epoch=899.ckpt\"\n",
    "model = MUNIT.load_from_checkpoint(path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "encoder1 = model.generator.encoder1\n",
    "encoder2 = model.generator.encoder2\n",
    "decoder1 = model.generator.decoder1\n",
    "decoder2 = model.generator.decoder2\n",
    "\n",
    "img_contents = artificial.to(model.device)\n",
    "real_contents, _ = encoder1(img_contents)\n",
    "\n",
    "\n",
    "grid = []\n",
    "for i, content in enumerate(real_contents):\n",
    "    # for each content create n_random_styles\n",
    "    row = th.cat(\n",
    "        [th.unsqueeze(content, 0) for _ in range(len(random_styles))], dim=0)\n",
    "    imgs = decoder2(row.to(model.device),\n",
    "                    random_styles)\n",
    "\n",
    "    # a row consists of content image and its derivatives\n",
    "    grid.append(img_contents[i].detach().cpu().clone())\n",
    "    grid += [img.detach().cpu().clone()\n",
    "             for img in imgs]\n",
    "\n",
    "# save everyting\n",
    "grid = torchvision.utils.make_grid(\n",
    "    grid, nrow=len(random_styles) + 1, pad_value=1, normalize=True, range=(-1, 1))\n",
    "torchvision.utils.save_image(grid, \"figs/test_munit1.png\")\n",
    "\n",
    "\n",
    "\n",
    "# munit2\n",
    "path = \"experiments/munit_version_2_larger_image_corrected_500/lightning_logs/version_0/checkpoints/epoch=499.ckpt\"\n",
    "model = MUNIT.load_from_checkpoint(path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "encoder1 = model.generator.encoder1\n",
    "encoder2 = model.generator.encoder2\n",
    "decoder1 = model.generator.decoder1\n",
    "decoder2 = model.generator.decoder2\n",
    "\n",
    "img_contents = artificial.to(model.device)\n",
    "real_contents, _ = encoder1(img_contents)\n",
    "\n",
    "\n",
    "grid = []\n",
    "for i, content in enumerate(real_contents):\n",
    "    # for each content create n_random_styles\n",
    "    row = th.cat(\n",
    "        [th.unsqueeze(content, 0) for _ in range(len(random_styles))], dim=0)\n",
    "    imgs = decoder2(row.to(model.device),\n",
    "                    random_styles)\n",
    "\n",
    "    # a row consists of content image and its derivatives\n",
    "    grid.append(img_contents[i].detach().cpu().clone())\n",
    "    grid += [img.detach().cpu().clone()\n",
    "             for img in imgs]\n",
    "\n",
    "# save everyting\n",
    "grid = torchvision.utils.make_grid(\n",
    "    grid, nrow=len(random_styles) + 1, pad_value=1, normalize=True, range=(-1, 1))\n",
    "torchvision.utils.save_image(grid, \"figs/test_munit2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5d5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
