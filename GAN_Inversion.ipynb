{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lpips\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import cv2\n",
    "\n",
    "from scripts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc57a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important params\n",
    "checkpoint_dir = \"experiments/dcgan_disc_double_params_padding_reflect_lr_scheduler/lightning_logs/version_0/checkpoints/epoch=899.ckpt\"\n",
    "save_path = \"experiments/gan_inversion/dcgan_disc_double_params_padding_reflect_lr_scheduler_epoch=899\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# optional\n",
    "bs = 64\n",
    "n_iter = 1000\n",
    "r, c = 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = GAN.load_from_checkpoint(checkpoint_dir)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake data loader\n",
    "\n",
    "# path names\n",
    "X_fake_path = os.path.join(save_path, \"X_fake.npy\")\n",
    "y_fake_path = os.path.join(save_path, \"y_fake.npy\")\n",
    "\n",
    "if not os.path.exists(X_fake_path) or not os.path.exists(y_fake_path):\n",
    "    y_fake = []\n",
    "    X_fake = []\n",
    "\n",
    "    with th.no_grad():\n",
    "        for i in range(10):\n",
    "            z = th.normal(0, 1, (1000, model.generator.latent_dim), device=model.device)\n",
    "            x = model.generator(z)\n",
    "\n",
    "            y_fake.append(z.detach().cpu())\n",
    "            X_fake.append(x.detach().cpu())\n",
    "\n",
    "    y_fake = th.cat(y_fake, axis=0)\n",
    "    X_fake = th.cat(X_fake, axis=0)\n",
    "\n",
    "    # save fake dataset\n",
    "    np.save(X_fake_path, X_fake.detach().cpu().numpy())\n",
    "    np.save(y_fake_path, y_fake.detach().cpu().numpy())\n",
    "else:\n",
    "    X_fake = th.tensor(np.load(X_fake_path))\n",
    "    y_fake = th.tensor(np.load(y_fake_path))\n",
    "dataset = TensorDataset(X_fake.detach().cpu(), y_fake.detach().cpu())\n",
    "dataloader_fake = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=2)\n",
    "\n",
    "y_fake.shape, X_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0786765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real data loader\n",
    "\n",
    "X_real_path = os.path.join(save_path, \"X_real.npy\")\n",
    "\n",
    "cars = np.load(\"../potsdam_data/potsdam_cars/cars.npy\", allow_pickle=True)\n",
    "\n",
    "if not os.path.exists(X_real_path):\n",
    "    X_real = []\n",
    "    for car in tqdm.tqdm(cars):\n",
    "        car_resized = cv2.resize(car, (c, r)).transpose(2, 0, 1).astype(np.float32)\n",
    "\n",
    "        # scaling -1, 1\n",
    "        car_resized = 2*(car_resized / 255) - 1\n",
    "\n",
    "        # append\n",
    "        X_real.append(np.expand_dims(car_resized, axis=0))\n",
    "\n",
    "    X_real = np.concatenate(X_real, axis=0)\n",
    "\n",
    "    # save real dataset\n",
    "    np.save(X_real_path, X_real)\n",
    "else:\n",
    "    X_real = np.load(X_real_path)\n",
    "\n",
    "X_real = th.tensor(X_real, dtype=X_fake.dtype)\n",
    "\n",
    "dataset = TensorDataset(X_real, X_real)\n",
    "dataloader_real = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=2)\n",
    "\n",
    "X_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea057c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val data loader\n",
    "\n",
    "X_real_path = os.path.join(save_path, \"X_real_val.npy\")\n",
    "\n",
    "cars = np.load(\"../potsdam_data/potsdam_cars_val/cars.npy\", allow_pickle=True)\n",
    "\n",
    "if not os.path.exists(X_real_path):\n",
    "    X_real = []\n",
    "    for car in cars:\n",
    "        car_resized = cv2.resize(car, (c, r)).transpose(2, 0, 1).astype(np.float32)\n",
    "        \n",
    "        # scaling -1, 1\n",
    "        car_resized = 2*(car_resized / 255) - 1\n",
    "\n",
    "        # append\n",
    "        X_real.append(np.expand_dims(car_resized, axis=0))\n",
    "        \n",
    "    X_real = np.concatenate(X_real, axis=0)\n",
    "\n",
    "    # save real dataset\n",
    "    np.save(X_real_path, X_real)\n",
    "else:\n",
    "    X_real = np.load(X_real_path)\n",
    "\n",
    "X_real = th.tensor(X_real, dtype=X_fake.dtype)\n",
    "\n",
    "dataset = TensorDataset(X_real, X_real)\n",
    "dataloader_val = DataLoader(dataset, batch_size=2*bs, shuffle=True, num_workers=2)\n",
    "\n",
    "X_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2697ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EncoderLatent(generator).cuda()\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_latent = th.nn.L1Loss(reduction=\"mean\")\n",
    "loss_rec = th.nn.L1Loss(reduction=\"mean\")\n",
    "loss_rec = lpips.LPIPS(net='vgg').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52531605",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(net.cuda(), (3, 32, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30612ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "\n",
    "learning_curve = []\n",
    "learning_curve_val = []\n",
    "n_iter = 300\n",
    "\n",
    "for i in range(n_iter):\n",
    "\n",
    "    x_fake, z_fake = next(iter(dataloader_fake))\n",
    "    x_real, _ = next(iter(dataloader_real))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    z_fake_, x_fake_ = net.forward(x_fake.cuda())\n",
    "    _, x_real_ = net.forward(x_real.cuda())\n",
    "\n",
    "\n",
    "    loss_1 = loss_latent(z_fake_, z_fake.cuda())\n",
    "    loss_2 = th.mean(loss_rec(x_fake_, x_fake.cuda(), normalize=True))\n",
    "    loss_3 = th.mean(loss_rec(x_real_, x_real.cuda(), normalize=True))\n",
    "    total_loss = loss_1 + loss_2 + loss_3\n",
    "    \n",
    "    learning_curve.append(total_loss.item())\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration \", i, \"loss\", learning_curve[-1])\n",
    "    \n",
    "    if i % 90 == 0:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = g['lr']*0.1\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        with th.no_grad():\n",
    "            net.eval()\n",
    "            l = 0\n",
    "            for x_val, _ in dataloader_val:\n",
    "                _, x_val_ = net.forward(x_val.cuda())\n",
    "                l += th.mean(loss_rec(x_val_, x_val.cuda(), normalize=True)).item()\n",
    "            print(\"iteration \", i, \"val loss\", l)\n",
    "            learning_curve_val.append(l)\n",
    "            net.train()\n",
    "        th.save(net.state_dict(), os.path.join(save_path, f\"iter={i}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learning_curve)\n",
    "plt.plot([100*i for i in range(len(learning_curve_val))], learning_curve_val)\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfd360",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_iter = 300\n",
    "net.load_state_dict(th.load(os.path.join(save_path, f\"iter={best_model_iter}.pkl\")))\n",
    "net.cuda()\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "with th.no_grad():\n",
    "    z = th.normal(0, 1, (1, model.generator.latent_dim), device=model.device)\n",
    "    x_orig = model(z)\n",
    "\n",
    "    z_rec, _ = net.forward(x_orig)\n",
    "    x_rec = model.generator(z_rec)\n",
    "\n",
    "# rescale\n",
    "x_rec = (np.squeeze(x_rec.detach().cpu().numpy()).transpose(1, 2, 0) + 1) / 2\n",
    "x_orig = (np.squeeze(x_orig.detach().cpu().numpy()).transpose(1, 2, 0) + 1) / 2\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(x_rec)\n",
    "plt.title(\"Reconstructed (Fake)\", fontsize=12)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(x_orig)\n",
    "plt.title(\"Original (Fake)\", fontsize=12)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f5b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "x_val, _ = next(iter(dataloader_val))\n",
    "\n",
    "x_orig = th.unsqueeze(x_val[i], axis=0)\n",
    "z_rec, _ =  net.forward(x_orig.cuda())\n",
    "x_rec = model.generator(z_rec)\n",
    "\n",
    "# rescale\n",
    "x_rec = (np.squeeze(x_rec.detach().cpu().numpy()).transpose(1, 2, 0) + 1) / 2\n",
    "x_orig = (np.squeeze(x_orig.detach().cpu().numpy()).transpose(1, 2, 0) + 1) / 2\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(x_rec)\n",
    "plt.title(\"Reconstructed (Fake)\", fontsize=16)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(x_orig)\n",
    "plt.title(\"Original (Real)\", fontsize=16)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce659ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21083f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a21aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
